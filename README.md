# <p align="center">Домашнее задание к занятию "Репликация и масштабирование. Часть 2"</p>

**SDBSQL-46 / Ларионов Сергей**

---

## Задание 1: Различия master-slave и master-master

### Активный master‑сервер и пассивный репликационный slave‑сервер

Основные преимущества:
- Отказоустойчивость. Пассивный slave‑сервер выступает «горячим резервом»: в случае сбоя master‑сервера можно оперативно переключиться на реплику, минимизируя время простоя.
- Безопасность данных. Репликация обеспечивает постоянное копирование данных на slave‑сервер, что снижает риск потери информации при аппаратных или программных сбоях.
- Возможность резервного копирования без нагрузки на основной сервер. Резервные копии можно снимать с slave‑сервера, не замедляя работу основного приложения.
- Простота архитектуры. Схема с одним master и одним slave относительно легка в настройке и мониторинге, требует меньше ресурсов на сопровождение.
- Тестирование и отладка. На slave‑сервере можно разворачивать тестовые среды, анализировать данные или отрабатывать обновления, не затрагивая рабочую систему.

### Master‑сервер и несколько slave‑серверов

Основные преимущества:
- Горизонтальное масштабирование чтения. Запросы на чтение (SELECT) можно распределять между несколькими slave‑серверами, существенно повышая общую производительность системы при высокой нагрузке.
- Повышенная отказоустойчивость. Наличие нескольких реплик снижает риск полной потери доступности: даже если один slave выйдет из строя, остальные продолжат обслуживать запросы.
- Географическое распределение. Slave‑серверы можно разместить в разных дата‑центрах или регионах, сокращая задержки для пользователей и обеспечивая локальный доступ к данным.
- Разделение нагрузок. Разные slave‑серверы могут быть оптимизированы для конкретных задач: один — для отчётов, другой — для аналитики, третий — для резервного копирования. Это предотвращает конкуренцию ресурсов.
- Плавное масштабирование. При росте нагрузки можно добавлять новые slave‑серверы без остановки основной системы, адаптируя инфраструктуру под текущие потребности.
- Снижение нагрузки на master. Большинство операций чтения переносится на реплики, что позволяет master‑серверу эффективнее обрабатывать записи (INSERT/UPDATE/DELETE) и поддерживать консистентность данных.
- Гибкость в обновлении. Можно поэтапно обновлять slave‑серверы, проверяя совместимость и производительность, прежде чем применять изменения к master.

### Сравнительная таблица преимуществ двух схем масштабирования:

| Критерий                            | Master + пассивный slave                                               | Master + несколько slave                                                                 |
|-------------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------------------------|
| **Отказоустойчивость**              | Базовый уровень: есть горячий резерв для переключения при сбоях master | Повышенный уровень: несколько реплик снижают риск полной потери доступности              |
| **Масштабирование чтения**          | Отсутствует: slave обычно не используется для клиентских запросов      | Высокое: запросы на чтение распределяются между репликами, что снимает нагрузку с master |
| **Нагрузка на master**              | Средняя: все операции чтения идут через master                         | Низкая: большинство операций чтения перенесено на slave‑серверы                          |
| **Резервное копирование**           | Возможно с slave без нагрузки на master                                | Возможно с любого из slave, что даёт гибкость в планировании бэкапов                     |
| **Географическое распределение**    | Ограничено: обычно одна реплика в том же или соседнем ЦОД              | Возможно: реплики размещаются в разных регионах для снижения задержек                    |
| **Сложность управления**            | Низкая: простая архитектура, легко мониторить и настраивать            | Средняя/высокая: требуется координация нескольких реплик, контроль консистентности       | 
| **Затраты на инфраструктуру**       | Низкие: минимум дополнительного оборудования                           | Средние/высокие: требуется несколько серверов и каналы связи                             |
| **Тестирование и аналитика**        | Возможно на slave, но в ограниченном объёме                            | Гибкое: разные slave можно выделить под конкретные задачи (отчёты, аналитика, тесты)     |
| **Время восстановления после сбоя** | Минимальное: переключение на готовую реплику                           | Минимальное: возможность выбора рабочей реплики для переключения                         |
| **Гибкость масштабирования**        | Низкая: добавление новых узлов не предусмотрено                        | Высокая: можно добавлять slave по мере роста нагрузки                                    |

### Примечания:

- В схеме «master + пассивный slave» slave чаще всего находится в режиме ожидания и активируется только при аварии.
- В схеме «master + несколько slave» реплики активно участвуют в обработке запросов, что требует настройки балансировки нагрузки и мониторинга синхронизации.
- Обе схемы предполагают асинхронную или синхронную репликацию, выбор которой влияет на консистентность данных и задержки.

---

## Задание 2: Конфигурация master-slave репликации MySQL

# Архитектура шардинга базы данных

Проект реализует комбинированную схему шардинга (вертикального и горизонтального) для базы данных с таблицами: `пользователи`, `книги`, `магазины`.

## 1. Цели шардинга

- Повышение производительности за счёт распределения нагрузки.
- Масштабируемость при росте объёма данных.
- Отказоустойчивость через репликацию.
- Снижение времени отклика для клиентских запросов.

## 2. Принципы построения системы

Основные принципы:
- **Независимость шардов** — каждый сегмент работает автономно.
- **Балансировка нагрузки** — равномерное распределение данных.
- **Репликация** — создание копий для отказоустойчивости.
- **Согласованность данных** — механизмы синхронизации.
- **Прозрачность доступа** — единое логическое представление БД.

## 3. Вертикальный шардинг


Каждая таблица размещается в отдельном шарде:

| Шард | Таблица | Назначение |
|------|--------|-----------|
| Шард 1 | `пользователи` | Профиль, учётные данные |
| Шард 2 | `книги` | Каталог, метаданные книг |
| Шард 3 | `магазины` | Адреса, контакты магазинов |

**Преимущества:**
- Изоляция часто используемых таблиц.
- Оптимизация под специфику данных.
- Упрощение резервного копирования.

## 4. Горизонтальный шардинг

Строки одной таблицы распределяются между шардами по ключу.

### Стратегии шардинга

- **`пользователи`** — по `user_id` (хеширование: `user_id % N`).
- **`книги`** — по `book_id` (диапазоны значений).
- **`магазины`** — по `city_id` (гео-принцип).

### Пример для таблицы `пользователи`

- Шард 1: `user_id % 3 == 0`
- Шард 2: `user_id % 3 == 1`
- Шард 3: `user_id % 3 == 2`

**Преимущества:**
- Масштабирование при росте данных.
- Параллельная обработка запросов.
- Снижение нагрузки на один сервер.

## 5. Комбинированная схема

Сочетает вертикальный и горизонтальный шардинг:

1. **Уровень 1 (вертикальный):** 3 шарда по таблицам.
2. **Уровень 2 (горизонтальный):** каждый шард разбивается на N подшардов.

**Пример структуры:**

- **Шард «пользователи»** → 3 подшарда (по `user_id`).
- **Шард «книги»** → 2 подшарда (по жанрам).
- **Шард «магазины»** → 4 подшарда (по регионам).

## 6. Архитектура серверов

### Режимы работы

- **Master (ведущий)** — приём запросов на запись/изменение.
- **Slave (ведомый)** — репликация данных, обработка запросов на чтение.
- **Balancer (балансировщик)** — распределение запросов между шардами.

### Топология

- 1 балансировщик (Nginx/HAProxy).
- 3 основных сервера (по одному на вертикальный шард).
- 2–3 реплики на каждый основной сервер.

## 7. Блоксхема системы

[Клиентские запросы]
        ↓
[Балансировщик нагрузки]
        ↓
┌───────────────────────────────┐
│     Вертикальный шардинг     │
├───────────────────────────────┤
│    Шард «пользователи»       │
│      ├─ Подшард 1          │
│      │   (user_id % 3 = 0) │
│      ├─ Подшард 2          │
│      │   (user_id % 3 = 1) │
│      └─ Подшард 3          │
│          (user_id % 3 = 2) │
├───────────────────────────────┤
│    Шард «книги»             │
│      ├─ Подшард 1          │
│      │   (жанр A–M)       │
│      └─ Подшард 2          │
│          (жанр N–Z)       │
├───────────────────────────────┤
│    Шард «магазины»          │
│      ├─ Подшард 1          │
│      │   (регион 1)        │
│      ├─ Подшард 2          │
│      │   (регион 2)        │
│      ├─ Подшард 3          │
│      │   (регион 3)        │
│      └─ Подшард 4          │
│          (регион 4)        │
└───────────────────────────────┘
        ↓
[Реплики (Slave) для каждого шарда]


## 8. Механизмы синхронизации

- **Репликация Master-Slave** — асинхронная передача данных.
- **Консистентность** — распределённые транзакции (2PC).
- **Мониторинг** — проверка целостности данных.

## 9. Обработка запросов

1. Клиент отправляет запрос балансировщику.
2. Балансировщик определяет целевой шард по ключу.
3. Запрос направляется на Master (запись) или Slave (чтение).
4. Результат возвращается клиенту.

## 10. Отказоустойчивость

- **Репликация** — минимум 1 копия данных.
- **Автопереключение** — Slave берёт роль Master при падении.
- **Резервное копирование** — ежедневные снапшоты шардов.

## 11. Масштабирование

- **Горизонтальное** — добавление новых подшардов.
- **Вертикальное** — увеличение ресурсов серверов.
- **Динамическое перераспределение** — миграция данных при перегрузке.


























---
Конфигурация включает server-id=1 и включённый бинарный лог:
```bash
server-id = 1
log_bin = /var/log/mysql/mysql-bin.log
max_binlog_size = 100M
```

<img src="img/image1.png" alt="Конфигурация master" width="650"/>


```bash
text
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| server_id     | 2     |
+---------------+-------+
```

---
